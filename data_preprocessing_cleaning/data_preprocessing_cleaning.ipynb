{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing et cleaning des données\n",
    "L'objectif de ce notebook est de nettoyer et process la donnée avant traitement et anayse. "
   ],
   "id": "7d5aab93d70f1fec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Consignes:\n",
    "\n",
    "Use Spark to clean and preprocess the data. Key steps include:\n",
    "- Handling missing values.\n",
    "- Removing duplicates.\n",
    "- Normalizing data formats (e.g., date formats, categorical variables).\n",
    "- Filtering irrelevant data"
   ],
   "id": "cc0c7456f2cc056e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:02.592554Z",
     "start_time": "2024-11-25T16:01:00.085698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install pyspark\n",
    "!pip install pyspark[sql]\n",
    "!pip install pyspark[pandas_on_spark] plotly\n",
    "!pip install pandas"
   ],
   "id": "7914cf15c12fa692",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: pyspark[sql] in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[sql]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[sql]) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[sql]) (18.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.15 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[sql]) (1.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[sql]) (1.16.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: pyspark[pandas_on_spark] in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (18.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.15 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (1.25.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[pandas_on_spark]) (1.16.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas) (1.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:02.598278Z",
     "start_time": "2024-11-25T16:01:02.595559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n"
   ],
   "id": "6aef66d1b8e257ae",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Data loading",
   "id": "a2c46344dd946827"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we create a Spark session",
   "id": "4346b2c4ef7ce3a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:02.611135Z",
     "start_time": "2024-11-25T16:01:02.605798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.appName(\"BigDataProject\").getOrCreate()"
   ],
   "id": "fe647bbbf2119715",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:03.238496Z",
     "start_time": "2024-11-25T16:01:02.618652Z"
    }
   },
   "source": [
    "# Load CSV data\n",
    "file_path = \"../ecommerce_data_with_trends.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Save DataFrame as temporary SQL table\n",
    "data.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# Show first lines\n",
    "spark.sql(\"SELECT * FROM transactions LIMIT 5\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------+-----------------+-------------+--------------------+--------------------+------+--------+------------+\n",
      "|      transaction_id|           timestamp|customer_id| customer_name|             city|customer_type|        product_name|            category| price|quantity|total_amount|\n",
      "+--------------------+--------------------+-----------+--------------+-----------------+-------------+--------------------+--------------------+------+--------+------------+\n",
      "|TX_89a20095-f7be-...|2023-10-30 03:01:...|       6933|    David Hays|      New Sabrina|          B2C|Furniture Product_10|Home & Kitchen > ...|246.08|       4|      984.32|\n",
      "|TX_a6b15a50-47b9-...|2023-10-30 03:06:...|       9328| Adam Oconnell|East Katherineton|          B2C|Non-Fiction Produ...| Books > Non-Fiction| 792.3|       4|      3169.2|\n",
      "|TX_abdde2cb-3752-...|2023-10-30 03:06:...|       6766|   Jerry Brown|         Lukefort|          B2B|   Bedding Product_1|Home & Kitchen > ...|685.73|      40|     27429.2|\n",
      "|TX_ba162310-0807-...|2023-10-30 03:06:...|       9111|Craig Martinez|    South Leonard|          B2B|    Shoes Product_11|     Fashion > Shoes|404.96|      96|    38876.16|\n",
      "|TX_60ec44fd-2172-...|2023-10-30 03:08:...|       1763|    David Wood|      Jacksonstad|          B2B|Supplements Produ...|Health & Personal...|927.67|      35|    32468.45|\n",
      "+--------------------+--------------------+-----------+--------------+-----------------+-------------+--------------------+--------------------+------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Data Preprocessing",
   "id": "f9cdc550619b45a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:03.278471Z",
     "start_time": "2024-11-25T16:01:03.259517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove rows containing NULL values\n",
    "data_cleaned = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions\n",
    "    WHERE transaction_id IS NOT NULL\n",
    "      AND timestamp IS NOT NULL\n",
    "      AND customer_id IS NOT NULL\n",
    "      AND total_amount IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "data_cleaned.createOrReplaceTempView(\"transactions_cleaned\")\n"
   ],
   "id": "87a02a88ea027b1b",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:03.330557Z",
     "start_time": "2024-11-25T16:01:03.312286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Delete duplicate rows based on unique transaction id\n",
    "data_cleaned_no_duplicates = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT *\n",
    "    FROM transactions_cleaned\n",
    "\"\"\")\n",
    "\n",
    "data_cleaned_no_duplicates.createOrReplaceTempView(\"transactions_no_duplicates\")\n"
   ],
   "id": "9f1402341991a019",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:03.396241Z",
     "start_time": "2024-11-25T16:01:03.380289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filtrer les transactions non pertinentes\n",
    "data_filtered = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions_no_duplicates\n",
    "    WHERE total_amount > 0\n",
    "\"\"\")\n",
    "\n",
    "data_filtered.createOrReplaceTempView(\"transactions_filtered\")\n"
   ],
   "id": "51285a7963c71b7b",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Data Cleaning",
   "id": "49badd8dc2358b21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:03.417188Z",
     "start_time": "2024-11-25T16:01:03.403950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Timestamp format normalization\n",
    "# Convert timestamp column o format yyyy-MM-dd HH:mm:ss\n",
    "data_normalized = spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "           CAST(timestamp AS TIMESTAMP) AS normalized_timestamp\n",
    "    FROM transactions_filtered\n",
    "\"\"\")\n",
    "\n",
    "data_normalized.createOrReplaceTempView(\"transactions_normalized\")\n"
   ],
   "id": "141f56480c9c5f3f",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:03.442994Z",
     "start_time": "2024-11-25T16:01:03.425348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize customer types (uppercase, space removal)\n",
    "data_categorized = spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "           UPPER(TRIM(customer_type)) AS normalized_customer_type\n",
    "    FROM transactions_normalized\n",
    "\"\"\")\n",
    "\n",
    "data_categorized.createOrReplaceTempView(\"transactions_categorized\")\n"
   ],
   "id": "df297b3400e5a2f2",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Export processed and cleaned data",
   "id": "e7fb56a1ca8e7b1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:01:06.542903Z",
     "start_time": "2024-11-25T16:01:03.452171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"../preprocessed_data\"\n",
    "data_filtered.write.csv(output_path, header=True)\n",
    "\n",
    "print(f\"Preprocessed and cleaned data saved at {output_path}\")\n"
   ],
   "id": "148e988711311984",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and cleaned data saved at ../preprocessed_data\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
