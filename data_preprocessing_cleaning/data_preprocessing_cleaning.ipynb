{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5aab93d70f1fec",
   "metadata": {},
   "source": [
    "# Preprocessing et cleaning des données\n",
    "L'objectif de ce notebook est de nettoyer et process les données avant traitement et analyse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c7456f2cc056e",
   "metadata": {},
   "source": [
    "### Consignes:\n",
    "\n",
    "Use Spark to clean and preprocess the data. Key steps include:\n",
    "- Handling missing values.\n",
    "- Removing duplicates.\n",
    "- Normalizing data formats (e.g., date formats, categorical variables).\n",
    "- Filtering irrelevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7f981",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7914cf15c12fa692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:02.138386Z",
     "start_time": "2024-11-25T16:13:00.900459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: pyspark[sql] in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[sql]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[sql]) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[sql]) (18.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.15 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pyspark[sql]) (1.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from pandas>=1.0.5->pyspark[sql]) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samya\\onedrive\\documents\\epita\\ing3\\big data\\projet\\big-data-pipeline-group-studio-de-felice\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[sql]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install pyspark[sql]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6aef66d1b8e257ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:02.144420Z",
     "start_time": "2024-11-25T16:13:02.141391Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, trim\n",
    "from pyspark.sql.functions import to_timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c46344dd946827",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346b2c4ef7ce3a8",
   "metadata": {},
   "source": [
    "First we create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe647bbbf2119715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:02.158345Z",
     "start_time": "2024-11-25T16:13:02.152957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.appName(\"BigDataProject\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:02.793848Z",
     "start_time": "2024-11-25T16:13:02.166371Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- customer_type: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+-----------+--------------+-----------------+-------------+--------------------+--------------------+------+--------+------------+\n",
      "|      transaction_id|           timestamp|customer_id| customer_name|             city|customer_type|        product_name|            category| price|quantity|total_amount|\n",
      "+--------------------+--------------------+-----------+--------------+-----------------+-------------+--------------------+--------------------+------+--------+------------+\n",
      "|TX_89a20095-f7be-...|2023-10-30 03:01:...|       6933|    David Hays|      New Sabrina|          B2C|Furniture Product_10|Home & Kitchen > ...|246.08|       4|      984.32|\n",
      "|TX_a6b15a50-47b9-...|2023-10-30 03:06:...|       9328| Adam Oconnell|East Katherineton|          B2C|Non-Fiction Produ...| Books > Non-Fiction| 792.3|       4|      3169.2|\n",
      "|TX_abdde2cb-3752-...|2023-10-30 03:06:...|       6766|   Jerry Brown|         Lukefort|          B2B|   Bedding Product_1|Home & Kitchen > ...|685.73|      40|     27429.2|\n",
      "|TX_ba162310-0807-...|2023-10-30 03:06:...|       9111|Craig Martinez|    South Leonard|          B2B|    Shoes Product_11|     Fashion > Shoes|404.96|      96|    38876.16|\n",
      "|TX_60ec44fd-2172-...|2023-10-30 03:08:...|       1763|    David Wood|      Jacksonstad|          B2B|Supplements Produ...|Health & Personal...|927.67|      35|    32468.45|\n",
      "+--------------------+--------------------+-----------+--------------+-----------------+-------------+--------------------+--------------------+------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV data\n",
    "file_path = \"../ecommerce_data_with_trends.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show schema, first lines and total number of initial rows\n",
    "data.printSchema()\n",
    "data.show(5)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdc550619b45a1",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda0c32",
   "metadata": {},
   "source": [
    "We start with filtering NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df6ae00ea1327ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:03.119371Z",
     "start_time": "2024-11-25T16:13:02.803272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines after NULL values filtering : 1000000\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = data.dropna(subset=[\"transaction_id\", \"timestamp\", \"customer_id\", \"total_amount\"])\n",
    "\n",
    "# Checking\n",
    "print(f\"Number of lines after NULL values filtering : {data_cleaned.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf8215",
   "metadata": {},
   "source": [
    "We then drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4822143288bed040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:05.406568Z",
     "start_time": "2024-11-25T16:13:03.139413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines after duplicate filtering : 1000000\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = data_cleaned.dropDuplicates()\n",
    "\n",
    "# Checking\n",
    "print(f\"Number of lines after duplicate filtering : {data_cleaned.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6823a6",
   "metadata": {},
   "source": [
    "We finally conserve only relevant data (here transactions whose total amount is strictly positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa4c77ecc7425b55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:07.486514Z",
     "start_time": "2024-11-25T16:13:05.427335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines after unnecessary lines filtering : 1000000\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = data_cleaned.filter(data_cleaned[\"total_amount\"] > 0)\n",
    "\n",
    "# Checking\n",
    "print(f\"Number of lines after unnecessary lines filtering : {data_cleaned.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49badd8dc2358b21",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b3610",
   "metadata": {},
   "source": [
    "We first format the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "131a02bd9c27b563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:09.482184Z",
     "start_time": "2024-11-25T16:13:07.515859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           timestamp|\n",
      "+--------------------+\n",
      "|2023-10-30 03:29:...|\n",
      "|2023-10-30 03:59:...|\n",
      "|2023-10-30 04:15:...|\n",
      "|2023-10-30 07:27:...|\n",
      "|2023-10-30 07:53:...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = data_cleaned.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Checking\n",
    "data_cleaned.select(\"timestamp\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee721f91",
   "metadata": {},
   "source": [
    "We then format customer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67c420a4ae9d1bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:11.689006Z",
     "start_time": "2024-11-25T16:13:09.538812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|customer_type|\n",
      "+-------------+\n",
      "|          b2c|\n",
      "|          b2b|\n",
      "|          b2b|\n",
      "|          b2c|\n",
      "|          b2c|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = data_cleaned.withColumn(\"customer_type\", lower(trim(data_cleaned[\"customer_type\"])))\n",
    "\n",
    "# Checking\n",
    "data_cleaned.select(\"customer_type\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb56a1ca8e7b1f",
   "metadata": {},
   "source": [
    "## 4. Export processed and cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2deb4f9c7c51af94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:13:17.505634Z",
     "start_time": "2024-11-25T16:13:11.752681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and cleaned data saved at ../preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../preprocessed_data\"\n",
    "data_cleaned.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Preprocessed and cleaned data saved at {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
